{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.2.1-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.3\n",
      "  Using cached py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\py4j\\clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\rezaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"d:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\py4j\\clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#create spark context\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39mgetOrCreate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccident\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#create sql context\u001B[39;00m\n",
      "File \u001B[1;32md:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\pyspark\\context.py:392\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 392\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32md:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\pyspark\\context.py:146\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    144\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway\u001B[38;5;241m=\u001B[39mgateway, conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 146\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparkHome\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpyFiles\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatchSize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjsc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofiler_cls\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[39;00m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[1;32md:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\pyspark\\context.py:209\u001B[0m, in \u001B[0;36mSparkContext._do_init\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvironment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    208\u001B[0m \u001B[38;5;66;03m# Create the Java SparkContext through Py4J\u001B[39;00m\n\u001B[1;32m--> 209\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc \u001B[38;5;241m=\u001B[39m jsc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[39;00m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;241m=\u001B[39m SparkConf(_jconf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39mconf())\n",
      "File \u001B[1;32md:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\pyspark\\context.py:329\u001B[0m, in \u001B[0;36mSparkContext._initialize_context\u001B[1;34m(self, jconf)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_initialize_context\u001B[39m(\u001B[38;5;28mself\u001B[39m, jconf):\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;124;03m    Initialize SparkContext in function to allow subclass specific initialization\u001B[39;00m\n\u001B[0;32m    328\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mJavaSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjconf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1579\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1580\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1581\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1582\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1584\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1585\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1586\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1588\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1589\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32md:\\users\\rezaz\\pycharmprojects\\bigdata_introduction\\venv\\lib\\site-packages\\py4j\\protocol.py:334\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m                 \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 334\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    335\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    336\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28mtype\u001B[39m \u001B[38;5;241m=\u001B[39m answer[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mPy4JError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext"
     ]
    }
   ],
   "source": [
    "#import necessary packages\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "#create spark context\n",
    "SparkContext.getOrCreate()\n",
    "sc = SparkContext.getOrCreate(\"accident\")\n",
    "\n",
    "#create sql context\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the csv files into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crashes = sqlCtx.read.option('header','true').csv('data/Traffic_Crashes_-_Crashes.csv')\n",
    "vehicles = sqlCtx.read.option('header','true').csv('data/Traffic_Crashes_-_Vehicles.csv')\n",
    "peoples = sqlCtx.read.option('header','true').csv('data/Traffic_Crashes_-_People.csv')\n",
    "print(type(crashes))\n",
    "peoples.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the ratio of number of crashes where the person involved was using cell phone to that where the person was not using the cell phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peoples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mpeoples\u001B[49m\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPERSON_DATA\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m groupDF \u001B[38;5;241m=\u001B[39m sqlCtx\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT DRIVER_ACTION, count(*) from PERSON_DATA group by DRIVER_ACTION\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m groupDF\u001B[38;5;241m.\u001B[39msort(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount(1)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDRIVER_ACTION\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m30\u001B[39m, truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'peoples' is not defined"
     ]
    }
   ],
   "source": [
    "peoples.createOrReplaceTempView(\"PERSON_DATA\")\n",
    "groupDF = sqlCtx.sql(\"SELECT DRIVER_ACTION, count(*) from PERSON_DATA group by DRIVER_ACTION\")\n",
    "groupDF.sort(\"count(1)\",\"DRIVER_ACTION\").show(30, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crashes that occurs because of phone :  2125\n",
      "crashes that occurs because of other :  409788\n",
      "ratio:  0.5185608168125958\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "phone = peoples.groupby(\"DRIVER_ACTION\").count().filter((peoples.DRIVER_ACTION == 'CELL PHONE USE OTHER THAN TEXTING') | (peoples.DRIVER_ACTION == 'TEXTING'))\n",
    "\n",
    "phone_crashes = phone.groupBy().sum('count').collect()[0][0]\n",
    "print(\"crashes that occurs because of phone : \", phone_crashes)\n",
    "\n",
    "no_phone = peoples.where((peoples.DRIVER_ACTION != \"UNKNOWN\") & (peoples.DRIVER_ACTION != \"NONE\"))\\\n",
    ".groupby(\"DRIVER_ACTION\")\\\n",
    ".count()\\\n",
    ".filter((peoples.DRIVER_ACTION != 'CELL PHONE USE OTHER THAN TEXTING') & (peoples.DRIVER_ACTION != 'TEXTING'))\n",
    "\n",
    "no_phone_crashes = no_phone.groupBy().sum('count').collect()[0][0]\n",
    "print(\"crashes that occurs because of other : \", no_phone_crashes)\n",
    "\n",
    "print(\"ratio: \", 100 * phone_crashes/no_phone_crashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*bring in package to convert age code to int for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|CAST(AGE AS INT)|count|\n",
      "+----------------+-----+\n",
      "|25              |26799|\n",
      "|26              |26603|\n",
      "|27              |26494|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "age = peoples.where((peoples.AGE > 0)).groupby(peoples.AGE.cast(IntegerType())).count().orderBy(\"count\", ascending=False).limit(3)\n",
    "age.show(120, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|CRASH_MONTH|count|\n",
      "+-----------+-----+\n",
      "|10         |56788|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month = crashes.select((crashes.CRASH_MONTH).cast(IntegerType()))\\\n",
    "                    .groupby(\"CRASH_MONTH\")\\\n",
    "                    .count()\\\n",
    "                    .orderBy(\"count\", ascending=False).limit(1)\n",
    "month.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|CRASH_DAY_OF_WEEK|count|\n",
      "+-----------------+-----+\n",
      "|1                |72848|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = crashes.select((crashes.CRASH_DAY_OF_WEEK).cast(IntegerType()))\\\n",
    "                    .groupby(\"CRASH_DAY_OF_WEEK\")\\\n",
    "                    .count()\\\n",
    "                    .orderBy(\"count\", ascending=True).limit(1)\n",
    "df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}